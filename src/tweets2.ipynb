{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here to\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['grid.color'] = 'lightgrey'\n",
    "# docker start sparkbook\n",
    "import pyspark as ps\n",
    "\n",
    "spark = (ps.sql.SparkSession.builder \n",
    "        .master(\"local[6]\") \n",
    "        .appName(\"case study\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json('../data/french_tweets.json')\n",
    "\n",
    "# create a temporary table for spark.sql queries\n",
    "df_json.createOrReplaceTempView('temp_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter results ofquery with content_filter\n",
    "#do counts for filtered results by hour/variable of choice\n",
    "#create df, transpose created df\n",
    "#apply counts to df\n",
    "#graph df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ho_testing(x):\n",
    "    '''\n",
    "    Inputs: x= list of lists   \n",
    "    output: a p_value \n",
    "    '''\n",
    "    \n",
    "    return stats.ttest_ind(x[0],x[1],alternative= \"greater\",equal_var =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets explore the text among possible senesitve tweets with media where the language is English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = spark.sql('''\n",
    "SELECT lang , text,created_at, possibly_sensitive, quoted_status.favorite_count, entities.media.type\n",
    "FROM temp1\n",
    "WHERE lang = 'en' and possibly_sensitive = \"True\";\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q3[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_filter(q):\n",
    "    '''\n",
    "    Inputs = list results from sql\n",
    "    Operations = clean text\n",
    "    Returns = dict \n",
    "    '''\n",
    "    content_df={}\n",
    "    for item in q:\n",
    "\n",
    "        time = item[2] \n",
    "        time2 = item[2][10:16]\n",
    "        status = str(item[3])\n",
    "        n_likes=str(item[4])\n",
    "        photo_check = str(item[5])\n",
    "        \n",
    "        if n_likes != 'None':\n",
    "            n_likes = int( ''.join(n_likes.strip(':') )) \n",
    "        if status != 'True' and status != 'False':\n",
    "            status = int(status)\n",
    "            \n",
    "        vals = [time,status,n_likes,photo_check]\n",
    "        \n",
    "        content_df[time2] = vals    \n",
    "        \n",
    "    return content_df      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting data for ho testing on media type and flagged yes \n",
    "dfcon= content_filter(q3[0:78600 & 79500:])\n",
    "\n",
    "media_type_counts = {'photo_likes':0}\n",
    "photo_likes = 'photo_likes'\n",
    "acum=0\n",
    "for k,v in dfcon.items():\n",
    "    media_type = v[3]\n",
    "    likes = str(v[2])\n",
    "    \n",
    "    if likes == 'None':\n",
    "        likes = 0\n",
    "    acum+=int(likes)\n",
    "    if media_type not in media_type_counts:\n",
    "        \n",
    "        media_type_counts[media_type]=1\n",
    "        media_type_counts['photo_likes'] += int(likes)\n",
    "        \n",
    "    else:\n",
    "        media_type_counts[media_type]+=1\n",
    "          \n",
    "print(media_type_counts,likes)\n",
    "    \n",
    "#go Through and collect / count occurances \n",
    "#of the 205 Flagged posts with media, 48 had a photo attachment and 133 did not\n",
    "#For the 50 likes from the 205 flagged posts with media 23 had likes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possibly sensitive results for all quoted_status\n",
    "q1 = spark.sql(\n",
    "'''\n",
    "SELECT created_at, possibly_sensitive\n",
    "FROM temp1; \n",
    "'''\n",
    ").collect()\n",
    "\n",
    "#tally tweets by hour. \n",
    "\n",
    "def times_hour(q):\n",
    "    by_hour2={}\n",
    "    for item in q:\n",
    "        time = item[0][10:13]\n",
    "        status = str(item[1])\n",
    "        \n",
    "        #text = item[2]\n",
    "        if time not in by_hour2:\n",
    "            by_hour2[time]={'yes':0, 'no':0, 'nao':0}\n",
    "        if len(status) < 0:\n",
    "            status = 'nao'\n",
    "        if time in by_hour2.keys():\n",
    "            if status == 'True':\n",
    "                by_hour2[time]['yes'] +=1\n",
    "            if status == 'False':\n",
    "                by_hour2[time]['no']+=1\n",
    "            else:\n",
    "                by_hour2[time]['nao']+=1\n",
    "    return by_hour2       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_hour = times_hour(q1[0:78600 & 79500:])\n",
    "print(type(by_hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print n_sensitive tweets per hour\n",
    "lst=[]\n",
    "for k,v in by_hour.items():\n",
    "    k_=int(k)\n",
    "    c=list(v.values())\n",
    "    lst.append((k_,c[0]))\n",
    "lst.sort(key = lambda x: x[0])\n",
    "\n",
    "x_val = [i[0] for i in lst]\n",
    "y_val = [i[1] for i in lst]\n",
    "plt.title('Hourly Counts: Possibly SensitiveTweets')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.plot(x_val,y_val)\n",
    "plt.plot(x_val,y_val, 'or')\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as Compared to all tweets per hour \n",
    "lst_all=[]\n",
    "for k,v in by_hour.items():\n",
    "    k_=int(k)\n",
    "    c=list(v.values())\n",
    "    lst_all.append((k_,c[2]))\n",
    "lst_all.sort(key = lambda x: x[0])\n",
    "print(lst_all)\n",
    "\n",
    "\n",
    "x_val = [i[0] for i in lst_all]\n",
    "y_val_all = [i[1] for i in lst_all]\n",
    "plt.title('Hourly Counts: All and Falsely Marked Sensitive')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.plot(x_val,y_val_all)\n",
    "plt.plot(x_val,y_val_all, 'or')\n",
    "plt.figure(figsize=(13,13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the first hypothesis being tested\n",
    "qq = spark.sql(\n",
    "'''\n",
    "SELECT quoted_status.retweet_count, temp1.entities.media.type\n",
    "FROM temp1\n",
    "WHERE quoted_status.possibly_sensitive= True;\n",
    "'''\n",
    ").collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq_ = [(item[0],str(item[1])) for item in qq] # dealing with NoneType \n",
    "print(qq_[:10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq1 = [int(item[0]) for item in qq_ if item[1] != 'None']\n",
    "sq2 = [int(item[0]) for item in qq_ if item[1] == 'None']\n",
    "print(sq1[:1], sq2[:10])\n",
    "\n",
    "'''\n",
    "Given there are no possibly sensitive tweets marked as True with media , there is insufficient evidence to reject the Ho.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can now scale our test upwords to see if, at any level, there is evidence that would support the alternative.\n",
    "\n",
    "Among Possibly sensitive tweets marked False Ho : retweet counts will not be significantly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the second hypothesis being tested\n",
    "q_false = spark.sql(\n",
    "'''\n",
    "SELECT quoted_status.retweet_count, temp1.entities.media.type\n",
    "FROM temp1\n",
    "WHERE quoted_status.possibly_sensitive= False;\n",
    "'''\n",
    ").collect() \n",
    "\n",
    "q_false_ = [(item[0],str(item[1])) for item in q_false] # dealing with NoneType \n",
    "fq1 = [int(item[0]) for item in q_false_  if item[1] != 'None']\n",
    "fq2 = [int(item[0]) for item in q_false_  if item[1] == 'None']\n",
    "fql3=[fq1,fq2]\n",
    "\n",
    "print(ho_testing(fql3))\n",
    "\n",
    "# Ttest_indResult(statistic=0.7119210156567528, pvalue=0.25414453508235946)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We still fail to reject the Ho\n",
    "\n",
    "What about at the next largest scale? Among all quoted satus.\n",
    "Ho Among retweets those with media will not have significantly different average retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is third\n",
    "\n",
    "retweet_all = spark.sql('''\n",
    "SELECT quoted_status.retweet_count, temp1.entities.media.type \n",
    "FROM temp1 ;\n",
    "''').collect()\n",
    "q3 = [(item[0],str(item[1])) for item in retweet_all if str(item[0]) != 'None'] # dealing with NoneType \n",
    "sq13 = [int(item[0]) for item in q3 if item[1] != 'None']\n",
    "sq23 = [int(item[0]) for item in q3 if item[1] == 'None']\n",
    "sql33=[sq13,sq23]\n",
    "\n",
    "#Ttest_indResult(statistic=0.8624315870353153, pvalue=0.4215514201308371)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We continue fail to reject the Ho. There Does not seem to be evidence to reject the hypothesis that media influences retweet status. \n",
    "\n",
    "-Does the trend of media not impacting tweet popularity continue when we switch our dependent variable to like_count ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_likes = spark.sql('''\n",
    "SELECT possibly_sensitive, quoted_status.favorite_count, temp1.entities.media.type \n",
    "FROM temp1\n",
    "WHERE possibly_sensitive=True;\n",
    "''').collect()\n",
    "san1 = pd.DataFrame(san_likes)\n",
    "san1.rename(columns = {0:'Sensitive', 1:'Like Count', 2:'Media' }, inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Given there are no possibly sensitive tweets marked as True with media , there is insufficient evidence to reject the Ho.\n",
    "'''"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can now scale our test upwords to see if, at any level, there is evidence that would support the alternative hypotheses.\n",
    "\n",
    "Among Possibly sensitive tweets marked False Ho : favorite counts will not be significantly different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second set of hypotheses | media impacts on like_count | CANT test = 'True' because there are No True with media\n",
    "san_false_likes = spark.sql('''\n",
    "SELECT possibly_sensitive, quoted_status.favorite_count, temp1.entities.media.type \n",
    "FROM temp1\n",
    "WHERE possibly_sensitive=False ;\n",
    "''').collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#among the possibly_senesitive tweets marked as False, those with media will not have sig different average like rates\n",
    "qq_1 = [(str(item[1]),str(item[2])) for item in san_false_likes if str(item[2]) != 'None']\n",
    "qq_2 = [(str(item[1]),str(item[2])) for item in san_false_likes if str(item[2]) == 'None'] # dealing with NoneType \n",
    " \n",
    "sq1 = [int(item[0]) for item in qq_1 if item[0] != 'None'] \n",
    "sq2 = [int(item[0]) for item in qq_2 if item[0] != 'None']\n",
    "sqlf3=[sq1,sq2]\n",
    "#Ttest_indResult(statistic=0.8022968610774406, pvalue=0.4529592583450547)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We still fail to reject the Ho\n",
    "\n",
    "What about at the next largest scale? Among all quoted satus.\n",
    "Ho Among retweets those with media will not have significantly different average retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_quote_likes = spark.sql('''\n",
    "SELECT quoted_status.favorite_count, temp1.entities.media.type \n",
    "FROM temp1;\n",
    "''').collect()\n",
    "qq_1 = [(str(item[0]),str(item[1])) for item in all_quote_likes if str(item[1]) != 'None']\n",
    "qq_2 = [(str(item[0]),str(item[1])) for item in all_quote_likes if str(item[1]) == 'None'] # dealing with NoneType \n",
    "sq1 = [int(item[0]) for item in qq_1 if item[0] != 'None']\n",
    "sq2 = [int(item[0]) for item in qq_2 if item[0] != 'None']\n",
    "sql3=[sq1,sq2]\n",
    "#Ttest_indResult(statistic=0.8062337547895845, pvalue=0.4508540499707302)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query to create DF and examine time of day for en senisitive tweets and time of day\n",
    "q4 = spark.sql('''\n",
    "SELECT lang , text,created_at, possibly_sensitive, quoted_status.favorite_count, entities.media.type\n",
    "FROM temp_view\n",
    "WHERE lang = 'en';\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing content_filter for q4 \n",
    "#clean strings\n",
    "\n",
    "def content_filter2(q):\n",
    "    '''\n",
    "    input = sql results\n",
    "    process rows, count likes, create dict\n",
    "    output = processed dict\n",
    "    '''\n",
    "    content_df2={}\n",
    "    counter = 0 \n",
    "    for item in q:\n",
    "        \n",
    "        #time = item[2][11:18]\n",
    "        hour_of_day=item[2][10:13] #Hour of day\n",
    "        flagg = str(item[3]) #Senesitive Status \n",
    "        n_likes=str(item[4]) #int or none \n",
    "        photo_check = str(item[5]) #none or no\n",
    "        \n",
    "        if n_likes != 'None':\n",
    "            n_likes = int( ''.join(n_likes.strip(':') )) \n",
    "            \n",
    "        if photo_check == 'None':\n",
    "            photo_check = 'No'\n",
    "\n",
    "        vals = [hour_of_day,flagg,n_likes,photo_check]\n",
    "        content_df2[counter] = vals    \n",
    "        counter +=1\n",
    "        \n",
    "    return content_df2   \n",
    "\n",
    "all_media = content_filter2(q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame(all_media)\n",
    "df_ = df.transpose()\n",
    "df_.rename(columns = {0:'Hour', 1:'Sensitive', 2:'Like Count', 3:'Media'}, inplace=True)\n",
    "df_['Type'] =  df_['Sensitive'] + df_['Media'] \n",
    "subset = df_[(df_['Like Count'] != 'None')]\n",
    "tup_lst = list(zip(subset['Type'].values, subset['Like Count'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media_d = {}\n",
    "for item in tup_lst:\n",
    "    stats = item[0]\n",
    "    likes = item[1]\n",
    "    if stats not in media_d:\n",
    "        media_d[stats] = likes\n",
    "    else:\n",
    "        media_d[stats]+= likes\n",
    "print(media_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset2 = subset[subset['Media']!= 'No']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# possibly sensitive results for all quoted_status\n",
    "spark.sql(\n",
    "'''\n",
    "SELECT quoted_status.possibly_sensitive, entities.media.type,\n",
    "       COUNT(quoted_status.favorite_count) AS fav_count, \n",
    "       AVG(quoted_status.favorite_count) AS avg_n_count,\n",
    "       stddev_samp(quoted_status.favorite_count) AS sd_n_retweets,\n",
    "       max(quoted_status.favorite_count) AS max_n_retweets,\n",
    "       MIN(quoted_status.favorite_count) AS min_n_retweets\n",
    "FROM temp_view\n",
    "GROUP BY quoted_status.possibly_sensitive AND entities.media.type; \n",
    "'''\n",
    ").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q5 = spark.sql('''\n",
    "SELECT possibly_sensitive, quoted_status.favorite_count, entities.media.type\n",
    "FROM temp1\n",
    "WHERE quoted_status.possibly_sensitive=True AND entities.media.type= 'photo';\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quoted_media_True = spark.sql('''\n",
    "SELECT quoted_status.possibly_sensitive, quoted_status.favorite_count, temp1.entities.media.type \n",
    "FROM temp1\n",
    "WHERE quoted_status.possibly_sensitive=True ;\n",
    "''').collect()\n",
    "\n",
    "#Among this data set of french tweets there is no relationship among possibly sensitive tweets with media and without media \n",
    "#Evidence from there being no retweets with media among sensitive tweets marked True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about senesitive tweets in general? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san = spark.sql('''\n",
    "SELECT possibly_sensitive, quoted_status.favorite_count, temp1.entities.media.type \n",
    "FROM temp1\n",
    "WHERE possibly_sensitive=True ;\n",
    "''').collect()\n",
    "sen_True_media = pd.DataFrame(san)\n",
    "sen_True_media.rename(columns = {0:'Sensitive', 1:'Like Count', 2:'Media' }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "san2 = spark.sql('''\n",
    "SELECT possibly_sensitive, quoted_status.favorite_count, temp1.entities.media.type \n",
    "FROM temp1\n",
    "WHERE possibly_sensitive=False ;\n",
    "''').collect()\n",
    "sen_False_media = pd.DataFrame(san2)\n",
    "sen_False_media.rename(columns = {0:'Sensitive', 1:'Like Count', 2:'Media' }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s11 = [row[1] for row in sen_True_media where row[2] == 'photo']\n",
    "s22 = [row[1] for row in sen_True_media where row[2] != 'photo']\n",
    "s3 = [s11,s22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending Lisa's Hypothesis to include images\n",
    "\n",
    "retweet = spark.sql('''\n",
    "SELECT possibly_sensitive, quoted_status.retweet_count, temp1.entities.media.type \n",
    "FROM temp1\n",
    "WHERE possibly_sensitive=True ;\n",
    "''').collect()\n",
    "rt = pd.DataFrame(retweet)\n",
    "rt.rename(columns = {0:'Sensitive', 1:'ReTweet Count', 2:'Media' }, inplace=True)\n",
    "\n",
    "s11 = [row[1] for row in rt if row[2] == 'photo']\n",
    "s22 = [row[1] for row in rt if row[2] != 'photo']\n",
    "s3 = [s11,s22]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here to\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['grid.color'] = 'lightgrey'\n",
    "# docker start sparkbook\n",
    "import pyspark as ps\n",
    "\n",
    "spark = (ps.sql.SparkSession.builder \n",
    "        .master(\"local[6]\") \n",
    "        .appName(\"case study\") \n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "en_stop = ['lbc','amp','au', 'aux', 'avec', 'ca' 'ce', 'ces', 'cest', 'dans', 'de', 'des', \n",
    "           'du', 'elle', 'en', 'et', 'eux', 'il', 'ils', 'je', 'la', \n",
    "           'le', 'les', 'leur', 'lui', 'ma', 'mais', 'me', 'm√™me', \n",
    "           'mes', 'moi', 'mon', 'ne', 'nos', 'notre', 'nous', 'on', \n",
    "           'ou', 'par', 'pas', 'pour', 'qu', 'que', 'qui', 'sa', 'se', \n",
    "           'ses', 'son', 'sur', 'ta', 'te', 'tes', 'toi', 'ton', 'tu', \n",
    "           'un', 'une', 'vos', 'votre', 'vous', 'c', 'd', 'j', 'l', '√†', \n",
    "           'm', 'n', 's', 't', 'y', '√©t√©', '√©t√©e', '√©t√©es', '√©t√©s', \n",
    "           '√©tant', '√©tante', '√©tants', '√©tantes', 'suis', 'es', 'est', \n",
    "           'sommes', '√™tes', 'sont', 'serai', 'seras', 'sera', 'serons', \n",
    "           'serez', 'seront', 'serais', 'serait', 'serions', 'seriez', \n",
    "           'seraient', '√©tais', '√©tait', '√©tions', '√©tiez', '√©taient', \n",
    "           'fus', 'fut', 'f√ªmes', 'f√ªtes', 'furent', 'sois', 'soit', \n",
    "           'soyons', 'soyez', 'soient', 'fusse', 'fusses', 'f√ªt', \n",
    "           'fussions', 'fussiez', 'fussent', 'ayant', 'ayante', \n",
    "           'ayantes', 'ayants', 'eu', 'eue', 'eues', 'eus', 'ai', 'as', \n",
    "           'avons', 'avez', 'ont', 'aurai', 'auras', 'aura', 'aurons', \n",
    "           'aurez', 'auront', 'aurais', 'aurait', 'aurions', 'auriez', \n",
    "           'auraient', 'avais', 'avait', 'avions', 'aviez', 'avaient', \n",
    "           'eut', 'e√ªmes', 'e√ªtes', 'eurent', 'aie', 'aies', 'ait', \n",
    "           'ayons', 'ayez', 'aient', 'eusse', 'eusses', 'e√ªt', 'eussions',\n",
    "           'eussiez', 'eussent','i', 'me', 'my', 'myself', 'we', 'our', \n",
    "           'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \n",
    "           \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', \n",
    "           'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \n",
    "           \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', \n",
    "           'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "           \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "           'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', \n",
    "           'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', \n",
    "           'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "           'with', 'about', 'against', 'between', 'into', 'through', 'during',\n",
    "           'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', \n",
    "           'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', \n",
    "           'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', \n",
    "           'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "           'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', \n",
    "           't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',\n",
    "           'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \n",
    "           \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", \n",
    "           'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \n",
    "           \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", \n",
    "           'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won',\n",
    "           \"won't\", 'wouldn', \"wouldn't\", 'si']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.json('../data/french_tweets.json')\n",
    "#df2_json = spark.read.json('/home/jovyan/work/Galva/caseStudy/Spark/spark-case-study/peer_extension/tweets.20150430-223406.json')\n",
    "# create a temporary table for spark.sql queries\n",
    "df_json.createOrReplaceTempView('temp_view')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_json2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-4dd89946de37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'temp2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_json2' is not defined"
     ]
    }
   ],
   "source": [
    "df_json2.createOrReplaceTempView('temp2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sql1 = spark.sql('''\n",
    "SELECT lang , text,created_at, possibly_sensitive, quoted_status.favorite_count\n",
    "FROM temp_view\n",
    "WHERE lang = 'en' and possibly_sensitive = \"True\";\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sql2 = spark.sql('''\n",
    "SELECT lang , text,created_at, possibly_sensitive, quoted_status.retweet_count\n",
    "FROM temp_view\n",
    "WHERE lang = 'en' and possibly_sensitive = \"False\";\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lang='en', text='The Old Blind Guitarist by Pablo Picasso https://t.co/qNxPnN3jI7', created_at='Wed Apr 26 13:34:36 +0000 2017', possibly_sensitive=True, favorite_count=None),\n",
       " Row(lang='en', text='Futur achat üòò https://t.co/DASepOFASa', created_at='Wed Apr 26 13:34:42 +0000 2017', possibly_sensitive=True, favorite_count=23),\n",
       " Row(lang='en', text='SCH - Comme si https://t.co/vaPuC4bxYp', created_at='Wed Apr 26 14:42:10 +0000 2017', possibly_sensitive=True, favorite_count=None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try and search/identify for Porn type text among tweets\n",
    "#among sensitive tweets with lots of re_tweets or likes_ examine more closely \n",
    "###perhaps deep dive among some volume of like_counts or re_tweet count top 5%? \n",
    "nlp_sql1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to filter symbols from nlp_sql1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(row):\n",
    "    to_remove = '''!_~^(){}@:'$%\"\\,-[]<>./?;@#&*'''''\n",
    "     \n",
    "    return ''.join([item for item in row if item not in to_remove]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_counts(sql_results):\n",
    "#     '''\n",
    "#     input = spark list_of_rows\n",
    "#     does = cleaning of string\n",
    "#     output= favorites,string\n",
    "#     '''\n",
    "#     countz = {}\n",
    "#     for index,item in enumerate(sql_results):\n",
    "#         clean =clean_text(item[1])\n",
    "#         #print(clean)\n",
    "#         favs = str(item[4])\n",
    "#         countz[index] = (favs,clean)\n",
    "#     #now have a dict of likes/tweet_text\n",
    "#     return countz\n",
    "\n",
    "# countz = get_counts(sql_results = nlp_sql1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countz2 = get_counts(sql_results = nlp_sql2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(countz[109]) -- > ('None', 'new Blackedcom trailer with the gorgeous MBlueEyesXxX \\n\n",
    "                                    # please RT and follow me and Markenna httpstcoNGWSjEZap4')\n",
    "# def clean_http():\n",
    "#     lst = []\n",
    "#     for k,v in countz2.items(): #countz or countz2\n",
    "#         v_ = v[1].split(' ')\n",
    "#         del v_[-1]\n",
    "#         for item in v_:\n",
    "#             lst.append(item)\n",
    "#     return lst\n",
    "# no_http2 = clean_http() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (no_http[2:10]) --> ['Blind', 'Guitarist', 'by', 'Pablo', 'Picasso', 'Futur', 'achat', 'üòò']\n",
    "#still not able to remove emojies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'countz2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7f986ac76249>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0msomefunct_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msomefunct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountz2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#countz or countz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# acum=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'countz2' is not defined"
     ]
    }
   ],
   "source": [
    "# Inputs for below could could be as follows\n",
    "# 202: ('None', 'so i went to a party tonight amp asked for the new shawns music and they played stitches WELL httpstcoWHoGPRcv3s'), \n",
    "# 203: ('50', '5 millions  httpstcoJGyUNbunZV')\n",
    "\n",
    "def somefunct(x):\n",
    "    word_dict = {}\n",
    "    counter =0\n",
    "\n",
    "    for k,v in x.items():\n",
    "        counter += 1\n",
    "        likez = v[0] \n",
    "        v=list(v[1].split(' ')) #convert text in list of words\n",
    "        #example_output for (v) --> ['The', 'Old', 'Blind', 'Guitarist', 'by', 'Pablo', 'Picasso', 'httpstcoqNxPnN3jI7']\n",
    "\n",
    "        for item in v: # for each word in the list \n",
    "            v_ = [item]\n",
    "            boozl =''\n",
    "            boozl = likez\n",
    "            if boozl == 'None': #workaround for dealing with None strings\n",
    "                likez = 'Nope' \n",
    "            if item not in word_dict:  \n",
    "                if boozl != 'Nope':\n",
    "                    word_dict[item] = list([boozl])\n",
    "            else:\n",
    "                if boozl != 'Nope':\n",
    "                    word_dict[item].append( boozl )\n",
    "    return word_dict\n",
    "\n",
    "somefunct_output = somefunct(countz2)  #countz or countz2\n",
    "\n",
    "# acum=0\n",
    "# for item in somefunct_output.values():\n",
    "#     unravel = len([x for x in item])\n",
    "#     acum+=unravel\n",
    "# print(acum) --> 602 #Count of words <with overlap however 4/26 --> or someother time remove overlap ?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Trying to make a network: each word is a node, each tweet is a node \n",
    "# connections would be links betweet the tweet and the word\n",
    "# Size of node =  number of likes \n",
    "# color of node indicates  = word or tweet (apply hue based on n_times word is used) Dark == more used, light == less used\n",
    "# Edge thickness degress separation ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringz = ' '.join([i for i in no_http])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringz2 = ' '.join([i for i in no_http2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len('üòò'))\n",
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Having challenges removing emoji \n",
    "# consulting Stackoverflow https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-460a41808392>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mtext2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_emoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# either stringz or stringz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "# https://stackoverflow.com/a/49146722/330558\n",
    "def remove_emoji(x):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', x)\n",
    "\n",
    " \n",
    "    \n",
    "text2 = remove_emoji(x) # either stringz or stringz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(stopwords=en_stop , \n",
    "                      width=1200, \n",
    "                      height=800, \n",
    "                      min_word_length=3,\n",
    "                      max_words=15).generate(text2)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.savefig('enFAL_cloud_top15.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-69290e5ac5ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mno_http22\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_http2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_http22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "def clean_http2(x):\n",
    "    lst = []\n",
    "    x_ = x.lower().split(' ')\n",
    "    del x_[-1]\n",
    "    return x_\n",
    "no_http22 = clean_http2(x)\n",
    "print(no_http22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 = spark.sql('''\n",
    "SELECT   text \n",
    "FROM temp_view\n",
    "WHERE lang = 'en' and possibly_sensitive = \"True\";\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = spark.sql('''\n",
    "SELECT   text \n",
    "FROM temp_view\n",
    "WHERE lang = 'en' and possibly_sensitive = \"False\";\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20020"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qw = spark.sql('''\n",
    "SELECT   text \n",
    "FROM temp_view\n",
    "WHERE lang='en';\n",
    "''').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41831"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text='The Old Blind Guitarist by Pablo Picasso https://t.co/qNxPnN3jI7'),\n",
       " Row(text='Futur achat üòò https://t.co/DASepOFASa'),\n",
       " Row(text='SCH - Comme si https://t.co/vaPuC4bxYp'),\n",
       " Row(text='new @PornDoePremium trailer with two perfect girls @Anna_swix @NaomiBenneta\\nplease RT and follow me Anna and Naomi https://t.co/1niJXq1W1f'),\n",
       " Row(text='Sch- Poupee Russe (clip offeciel ) https://t.co/V0cEuRKtx7'),\n",
       " Row(text=\"imagine being this jealous, can't relate https://t.co/fTvOuONvhP\"),\n",
       " Row(text='@MisGrace @LBC no news on this, lbc is a disgrace https://t.co/ytdplU1aYS ‚Ä¶'),\n",
       " Row(text='@Marsi132 @LBC @AlexSalmond @IainDale lbc no news = https://t.co/ytdplU1aYS ‚Ä¶'),\n",
       " Row(text='@_SEE_FUTURE @JonathanFrakes @gossippsychic @SkyNews @BBCNews @LBC did she see - Facts About 9/11 | Global Research‚Ä¶ https://t.co/7mzPtAgovT'),\n",
       " Row(text='@chunkymark @MichaelLee2009 Facts About 9/11 | Global Research - https://t.co/NwtNaVDlGF')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a function to process each row \n",
    "q2[0:10]\n",
    "#Clean each row\n",
    "##lowercase i.lower(), remove emojies remove_emojies() , remove http with clearn_http() , clean with clean_text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean2(sql_results):\n",
    "    '''\n",
    "    input = spark list_of_rows\n",
    "    does = cleaning of string\n",
    "    output= favorites,string\n",
    "    '''\n",
    "    countz_d = {}\n",
    "    for index,item in enumerate(sql_results):\n",
    "#         if index == 10:\n",
    "#             return countz_d \n",
    "        clean =clean_text(item) # prehaps cast clean to a list,\n",
    "        #clean2 = clean_text(clean) # using clean2 results in keeping screen names, which could be topics?\n",
    "        remove_emoj = remove_emoji(clean)\n",
    "        remove_http = clean_http2(remove_emoj)\n",
    "        remove_http_ =[item for item in remove_http if item != '@']\n",
    "        countz_d[index]=remove_http_ \n",
    "        \n",
    "    return countz_d  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = clean2(sql_results = q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20015\n"
     ]
    }
   ],
   "source": [
    "cleaned2 =clean2(sql_results = q3)\n",
    "\n",
    "d3={}\n",
    "for kk,vv in cleaned2.items():\n",
    "    lst2=[]\n",
    "    for wword in vv:\n",
    "        ww = list(wword) \n",
    "         \n",
    "        if len(ww) > 1 and ww[0] != '@':\n",
    "            v_ = clean_text(wword)\n",
    "            lst2.append(v_)\n",
    "        d3[kk]=lst2\n",
    "print(len(d3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "dd={}\n",
    "for k,v in cleaned.items():\n",
    "    lst=[]\n",
    "    for word in v:\n",
    "        w = list(word) \n",
    "         \n",
    "        if len(w) > 1 and w[0] != '@':\n",
    "            v_ = clean_text(word)\n",
    "            lst.append(v_)\n",
    "        dd[k]=lst\n",
    "print(len(dd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here to LDA is pyspark walkthrough\n",
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmx = spark.createDataFrame([[1, Vectors.dense([0.0, 1.0])],[2, SparseVector(2, {0: 1.0})],], [\"id\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|     features|\n",
      "+---+-------------+\n",
      "|  1|    [0.0,1.0]|\n",
      "|  2|(2,[0],[1.0])|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dmx.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(k=2, seed=1, optimizer=\"em\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDA_403808e1344e"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.setMaxIter(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9238\n"
     ]
    }
   ],
   "source": [
    "dd_str = ''\n",
    "acum=4\n",
    "while acum <5:\n",
    "    for item in dd.values():\n",
    "        item_ = ' '.join(item)\n",
    "        acum+=1\n",
    "        dd_str +=item_ + ' '\n",
    "print(len(dd_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scrapping Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7d433fd28995>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;31m# TfidfVectorizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1189\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \"string object received.\")\n",
      "\u001b[0;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "x= vectorizer.fit_transform(dd_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['This is the first document.','This document is the second document.','And this is the third one.','Is this the first document?']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5/3/21\n",
    "\n",
    "Trying to get LDA working -  \n",
    "using this for the TFIDF part https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html \n",
    "because the TFIDF methods from the LDA lecture is not working and I cant make sense of the all digit example in pyspark\n",
    "\n",
    "THEN can continue on within the prep data for LDA \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
